{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe586a2f-b02d-4250-88a4-146b65e2fd63",
   "metadata": {},
   "source": [
    "# Bayesian Machine Learning in Python: A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105f7249-3996-4474-8969-c143e0c84d5b",
   "metadata": {},
   "source": [
    "## Part 1 - Probability Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf07e77-a77f-488d-83f7-c76a502da9c0",
   "metadata": {},
   "source": [
    "### Probability Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51563a-3d85-428d-88e4-f26007cfb0dc",
   "metadata": {},
   "source": [
    "- Marginal Distributions:    p(A), p(B)\n",
    "- Joint Distribution:        p(A, B)\n",
    "- Conditional Distribution:  p(A|B), p(B|A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333b67f-2475-46c8-a4e7-3ddddc3716f4",
   "metadata": {},
   "source": [
    "#### Marginalization\n",
    "\n",
    "- How can you find the marginal distribution given the joint?\n",
    "\n",
    "$\n",
    "\\displaystyle p(A) = \\sum_B {p(A, B)} \\\\\n",
    "\\; \\\\\n",
    "\\displaystyle p(B) = \\sum_A {p(A, B)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd0db91-b4ed-4f90-9b91-d34d92216d41",
   "metadata": {},
   "source": [
    "#### Conditional Distributions\n",
    "\n",
    "- How can we calculate the conditional from the joint?\n",
    "\n",
    "$\n",
    "\\displaystyle p(A | B) = \\frac{p(A, B)}{p(B)} = \\frac{p(A, B)}{\\sum_A{p(A, B)}} \\\\\n",
    "\\; \\\\\n",
    "\\displaystyle p(B | A) = \\frac{p(A, B)}{p(A)} = \\frac{p(A, B)}{\\sum_B{p(A, B)}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4838ded-b99b-4b1e-a48d-a10dff412c40",
   "metadata": {},
   "source": [
    "#### Conditional from Conditional\n",
    "\n",
    "Given: $\\displaystyle  p(A | B), p(B) $<br>\n",
    "Want: $\\displaystyle  p(B | A) $<br>\n",
    "Recall: $\\displaystyle  p(A, B) = p(A | B)p(B) $ <br>\n",
    "<br>\n",
    "Derive: $\\displaystyle  p(B | A) = \\frac{p(A, B)}{p(A)} = \\frac{p(A, B)}{\\sum_B{p(A, B)}} = \\frac{p(A | B)p(B)}{\\sum_B{p(A | B)p(B)}} $ -> This is the **Bayes' Rule**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea5932a-dce2-4ea6-9641-3874a179f725",
   "metadata": {},
   "source": [
    "#### Discrete vs. Continuous Random Variables\n",
    "\n",
    "- p() is now a probability density (not a probability)\n",
    "- the rules still hold!\n",
    "\n",
    "Joint: $\\displaystyle  p(x, y) $\n",
    "Marginal: $\\displaystyle  p(x) = \\int{p(x, y)dy} \\;,\\; p(y) = \\int{p(x, y)dx}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b43b5-da9a-4bfe-be1c-a43a4008b919",
   "metadata": {},
   "source": [
    "#### Conditional Distributions\n",
    "\n",
    "$\n",
    "\\displaystyle p(x | y) = \\frac{p(x, y)}{p(y)} = \\frac{p(x, y)}{\\int{p(x, y)dx}}\\\\\n",
    "\\displaystyle p(y | x) = \\frac{p(x, y)}{p(x)} = \\frac{p(x, y)}{\\int{p(x, y)dy}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5519c11a-c5fa-4da4-97cb-303d1cf68b0e",
   "metadata": {},
   "source": [
    "#### Bayes' Rule\n",
    "\n",
    "$\n",
    "\\displaystyle p(x | y) = \\frac{p(y | x)p(x)}{\\int{p(y | x)p(x)dx}} \\\\\n",
    "\\displaystyle p(y | x) = \\frac{p(x | y)p(y)}{\\int{p(x | y)p(y)dy}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdf8ead-b78e-4f72-8ce9-703f9c305261",
   "metadata": {},
   "source": [
    "### Example Dataset\n",
    "\n",
    "|| CA | US | MX |\n",
    "| --- | --- | --- | --- |\n",
    "|Buy = True | 20 | 50 |10 |\n",
    "| Buy = False | 300 | 500 | 200 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffc4be-0e0a-460a-9b83-629f3ac24a79",
   "metadata": {},
   "source": [
    "##### Marginal\n",
    "\n",
    "* Let's find p(Country) first:\n",
    "\n",
    "$\n",
    "\\displaystyle p(Country = MX) = (10 + 200) / ((20+300) + (50+500) + (10+200)) = 210 / 1080 = 0.1944 \\\\\n",
    "\\displaystyle p(Country = US) = (50 + 500) / ((20+300) + (50+500) + (10+200)) = 550 / 1080 = 0.51 \\\\\n",
    "\\displaystyle p(Country = CA) = (20 + 300) / ((20+300) + (50+500) + (10+200)) = 320 / 1080 = 0.30 \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff89461-df99-4e35-81cf-d41a8f3bbd04",
   "metadata": {},
   "source": [
    "##### Joint Probabilities\n",
    "\n",
    "- there are 6 joint probabilities.\n",
    "- the number of probability values increases exponentially as we add more random variables\n",
    "\n",
    "$ Volume = |x_1| \\times |x_2| \\times |x_3| \\times ... \\times |x_N| $\n",
    "-> **Curse of dimensionality!**\n",
    "<br>\n",
    "\n",
    "$\n",
    "\\displaystyle p(Buy=True, CA) = 20 / 1080 = 0.019 \\\\\n",
    "\\displaystyle p(Buy=False, CA) = 300 / 1080 = 0.28 \\\\\n",
    "\\displaystyle p(Buy=True, US) = 50 / 1080 = 0.046 \\\\\n",
    "\\displaystyle p(Buy=False, US) = 500 / 1080 = 0.46 \\\\\n",
    "\\displaystyle p(Buy=True, MX) = 10 / 1080 = 0.009259 \\\\\n",
    "\\displaystyle p(Buy=False, MX) = 200 / 1080 = 0.18518\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f20c1-2a83-47e5-8695-f78340ef1966",
   "metadata": {},
   "source": [
    "##### Conditional Probabilities\n",
    "\n",
    "$\n",
    "p(Buy=True | CA) = 0.019/0.30 = 0.07 \\\\\n",
    "p(Buy=False | CA) = 0.28/0.30 = 0.93 \\\\\n",
    "p(Buy=True | US) = 0.046/0.51 = 0.09 \\\\\n",
    "p(Buy=False | US) = 0.46/0.51 = 0.91 \\\\\n",
    "p(Buy=True | MX) = 0.0093/0.19 = 0.0476 \\\\ \n",
    "p(Buy=False | MX) = 0.185/0.19 = 0.9523 \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43476ef-bb57-421a-932c-2330bb9c670b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/1080: 0.009259259259259259\n",
      " 200/1080: 0.18518518518518517 \n",
      " 210/1080: 0.19444444444444445\n",
      " buy&mx: 0.047619047619047616 \n",
      " nobuy&mx: 0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "print(f\"10/1080: {10/1080}\\n 200/1080: {200/1080} \\n 210/1080: {210/1080}\\n buy&mx: {(10/1080)/(210/1080)} \\n nobuy&mx: {(200/1080)/(210/1080)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f29bbda-3eaa-49cb-9649-69360fe2174c",
   "metadata": {},
   "source": [
    "##### Conditional Probabilities - Alternative Calculation\n",
    "\n",
    "$ \\displaystyle p(Buy=True | US) = \\frac{p(Buy=True, US)}{p(Country=US)} = \\frac{50/1080}{(50+500)/1080} = \\frac{50}{(50+500)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dead605-94bc-48bd-a873-30515d74c384",
   "metadata": {},
   "source": [
    "#### Example 2\n",
    "\n",
    "\n",
    "|| CA | US | MX |\n",
    "| --- | --- | --- | --- |\n",
    "| Buy=True | 20 | 50 | 10 |\n",
    "| Buy=False | 180 | 450 | 90 |\n",
    "\n",
    "$\n",
    "\\displaystyle p(Buy=True | CA) = 0.1 \\\\\n",
    "\\displaystyle p(Buy=True | US) = 0.1 \\\\\n",
    "\\displaystyle p(Buy=True | MX) = 0.1\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7debc1c6-28d9-4628-a33f-26f3b16090dc",
   "metadata": {},
   "source": [
    "### Independence\n",
    "\n",
    "* Intuitively: knowing the value of one random variable doesn't tell me anything about the other\n",
    "* Ex. A = Person 1 coin toss result\n",
    "* Ex. B = Person 2 coin toss result\n",
    "* Flipping the same (fair) coin twice is still independent.\n",
    "\n",
    "$ A \\perp B \\iff p(A, B) = p(A)p(B) $\n",
    "\n",
    "**\"Two random variables are independent if and only if their joint distribution p(A, B) is equal to the product of their marginal distributions p(A) times p(B).\"**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d727a2-84ef-4446-8897-d6edca3a4f85",
   "metadata": {},
   "source": [
    "#### Independence (conditional)\n",
    "\n",
    "- Suppose Buy and Country are independent\n",
    "\n",
    "$\n",
    "\\displaystyle Buy \\perp Country \\iff p(Buy, Country) = p(Buy)p(Country)  \\\\\n",
    "\\displaystyle p(Buy | Country) = \\frac{p(Buy, Country)}{p(Country)} = \\frac{p(Buy)p(Country)}{p(Country)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20aff6e-1050-4509-b2ca-63e59dbbc1b4",
   "metadata": {},
   "source": [
    "*continuing with Example 2*\n",
    "\n",
    "$\n",
    "\\displaystyle p(Buy) = (20 + 50 + 10)/(200 + 500 + 100) = 0.1\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827e97ff-f589-4a1c-b94e-84cb454a4bb7",
   "metadata": {},
   "source": [
    "### Simple Probability Exercise\n",
    "\n",
    "Assumption:\n",
    "* each coin toss in a sequence of coin tosses is independent\n",
    "* they are identically distributed:\n",
    "    - i.e. probability of heads is the same for each toss\n",
    "* iid = independent and identically distributed\n",
    "* the coin is a fair coin: p(H) = p(T) = 0.5\n",
    "* we plan to toss the coin 200 times\n",
    "* we have tossed 100 times so far, results:\n",
    "    - 80 heads , 20 tails\n",
    "\n",
    "**2 Typical answers:**\n",
    "\n",
    "1. Since it's a fair coin, we expect to get 100 heads and tails each.\n",
    "\n",
    "2. The past is fixed and they can't affect future predictions.\n",
    "    - 100 tosses left (expect to see 50 heads and tails each)\n",
    "    - Total heads 80+50 = 130\n",
    "    - Total tails 20+50 = 70\n",
    "    \n",
    "**The correct answer is 2.**\n",
    "- Coin tosses are independent, i.e. p(toss2 | toss1) = p(toss2)\n",
    "- the past is fixed\n",
    "- we can only predict the future outcomes\n",
    "- we therefore predict 50 *more* heads and 50 *more* tails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1951aa86-e9f7-40ee-b2b5-fb4e10f40117",
   "metadata": {},
   "source": [
    "#### The Gambler's Fallacy\n",
    "- The incorrect choice 1. is so common, it has a name\n",
    "- it is a false believe that things will \"balance out\" in the end\n",
    "- e.g. a gambler who has just lost several times believes it is *more likely* they will win next\n",
    "- the chance of losing th enext game is just as bad as it has always been."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d93ae7-8d1c-46b3-8e6f-fe4a5fb022f1",
   "metadata": {},
   "source": [
    "### The Monty Hall Problem\n",
    "\n",
    "- Famous problem in probability, inspired by a TV game show\n",
    "- TV show was *Let's Make a Deal*, host was *Monty Hall* - hence **The Monty Hall Problem**\n",
    "\n",
    "*How does it work?*\n",
    "\n",
    "1. contestant picks a door out of three, without opening it.\n",
    "2. Monty Hall opens a different door and reveals a goat.\n",
    "3. contestant can stay with the original choice or switch doors.\n",
    "---\n",
    "\n",
    "**Calculation**\n",
    "\n",
    "- let's assume contestant chooses door #1\n",
    "\n",
    "random variable C: which door the car is behind (C = 1, 2, 3)<br>\n",
    "random variable H: which door Monty Hall opens (assume H = 2)\n",
    "\n",
    "$\n",
    "p(H = 2 | C = 1) = 0.5 \\\\\n",
    "p(H = 2 | C = 2) = 0 \\\\\n",
    "p(H = 2 | C = 3) = 1\n",
    "$\n",
    "\n",
    "- what probability do we want?\n",
    "\n",
    "$ Want: p(C = 3|H = 2), p(C = 1| H = 2) $\n",
    "\n",
    "**Bayes rule -> way of switching around the givens (C & H)**\n",
    "\n",
    "$\n",
    "\\displaystyle p(C = 3|H = 2) = \\frac{p(H=2, C=3)}{p(H=2)} = \\frac{p(H=2|C=3)p(C=3)}{\\sum_{c=1}^3{p(H=2|C=c)p(C=c)}}  \\\\\n",
    "\\displaystyle p(C = 3|H = 2) = \\frac{p(H=2, C=3)p(C=3)}{p(H=2|C=1)p(C=1) + p(H=2|C=2)p(C=2) + p(H=2|C=3)p(C=3)}\n",
    "$\n",
    "\n",
    "- let's assume p(C) = 1/3\n",
    "\n",
    "$\n",
    "\\displaystyle p(C = 3|H = 2) = \\frac{\\frac{1}{3}}{\\frac{1}{2}\\frac{1}{3} + \\frac{1}{3}} = \\frac{2}{3}\n",
    "$\n",
    "\n",
    "- always switch!\n",
    "\n",
    "$\n",
    "\\displaystyle p(C = 3|H = 1) = \\frac{\\frac{1}{2}\\frac{1}{3}}{\\frac{1}{2}\\frac{1}{3} + \\frac{1}{3}} = \\frac{1}{3}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c553fa-c302-4260-aeae-a1c2b4888a1b",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "Maximum likelihood estimation is a technique for statistical modeling.<br>\n",
    "Imagine we have collected data from an experiment and would like to fit a model to that data.<br>\n",
    "Such a model usually comes with parameters. Our job now is to find the best parameters that they model the collected data as closely as possible.\n",
    "\n",
    "- modern example: Deep Learning / Neural Networks\n",
    "- the *learning* part is simply finding the best **parameters** to fit the **data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04378ef4-9549-4091-a7d9-8aff71f916ba",
   "metadata": {},
   "source": [
    "### The Bernoulli Distribution\n",
    "\n",
    "- Example:\n",
    "    - p(heads) = 0.6\n",
    "    - p(tails) = 0.4\n",
    "    \n",
    "Mathematical:\n",
    "- discrete random variable\n",
    "- PMF (probability mass function)\n",
    "\n",
    "$\n",
    "\\displaystyle p(x) = \\theta^x(1-\\theta)^{1-x}\n",
    "$\n",
    "\n",
    "*in this case, x can only be 0 or 1*<br>\n",
    "*$\\theta$ is the only parameter in this distribution*\n",
    "\n",
    "**Note:** $\\displaystyle  p(x = 1) = \\theta$\n",
    "\n",
    "$\n",
    "p(x = 1) = \\theta^1(1-\\theta)^{1-1} = \\theta \\\\\n",
    "p(x = 0) = \\theta^0(1-\\theta)^1 = 1 - \\theta \\\\\n",
    "\\; \\\\\n",
    "p(x = 1) + p(x = 0) = 1\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff332b4-691a-4d82-9f51-70853e3a9ac6",
   "metadata": {},
   "source": [
    "**Problem Setup**\n",
    "- suppose we have collected some data (flipped a coin several times)\n",
    "\n",
    "$\n",
    "\\displaystyle data = {x_1, x_2,...,x_N}\n",
    "$\n",
    "\n",
    "**Likelihood:**\n",
    "\n",
    "$\n",
    "\\displaystyle L(\\theta) = p(data | \\theta) = \\prod_{i=1}^N{p(x_i|\\theta)} = \\prod_{i=1}^N{\\theta^{x_i}(1-\\theta)^{1-x_i}}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea666f-37fd-4c1f-8c3b-5cc51dc198a6",
   "metadata": {},
   "source": [
    "### What is the Likelihood a Function of?\n",
    "\n",
    "- many people think it's **x**\n",
    "    - this is not correct!\n",
    "    - the **x**s are just the values we recorded in our experiment (1s and 0s)\n",
    "-the variable is $\\theta$\n",
    "\n",
    "**Example**\n",
    "\n",
    "$\n",
    "x_1 = 1, x_2 = 0, x_3 = 1 \\\\\n",
    "L(\\theta) = \\prod_{i=1}^N{\\theta^{x_i}(1-\\theta)^{1-x_i}} = \\theta^1 \\times (1-\\theta)^1 \\times \\theta^1\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b66e4-80c2-4574-b0fd-edfe96a28337",
   "metadata": {},
   "source": [
    "### Why is it called 'Maximum Likelihood'?\n",
    "\n",
    "- what value of $\\theta$ makes the data we collected **most probable**?\n",
    "- what values of $\\theta$ maximizes the likelihood?\n",
    "- ex. if we got 100 heads, 0 tails, $\\theta = 5%$ would not make sense!\n",
    "- it's more likely the probability is closer to 100%\n",
    "\n",
    "$\\displaystyle p(H) \\approx \\frac{N_H}{N_H + N_T} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0880cb08-5f4c-4e35-b562-abf91c6cd322",
   "metadata": {},
   "source": [
    "### Maximizing a Function\n",
    "\n",
    "- we want to take the derivative of L with respect to $\\theta$\n",
    "- we want to find the value of $\\theta$ that makes the derivative 0\n",
    "- we call this theta hat, hat is the symbol usually used for statistical estimates\n",
    "\n",
    "$\n",
    "\\displaystyle \\frac {dL}{d\\theta} = 0 \\\\\n",
    "\\;\\\\\n",
    "\\displaystyle \\hat{\\theta} = \\arg\\max_{\\theta}L(\\theta)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe7da15-14cc-444f-91b3-a4711dbbf2a6",
   "metadata": {},
   "source": [
    "### Log-Likelihood\n",
    "\n",
    "- it's better to take the log of the likelihood before differentiating\n",
    "- usually, the derivative is easier to solve \n",
    "- for Bernoulli, it's solvable both ways\n",
    "- why does this work?\n",
    "    - log() is monotonically increasing function\n",
    "    - whatever $\\theta$ maximizes L also maximizes logL\n",
    "    - pick any two values, where A > B -> is log(A) > log(B)?\n",
    "\n",
    "**Calculation**\n",
    "\n",
    "- take the log\n",
    "\n",
    "$\n",
    "\\displaystyle l(\\theta) = \\log{L(\\theta)} = \\log\\prod_{i=1}^N{\\theta^{x_i}(1-\\theta)^{1-x}}\\\\\n",
    "\\;\\\\\n",
    "\\displaystyle = \\sum_{i=1}^N{\\{x_i{\\log\\theta} + (1-x_i)\\log(1-\\theta)\\}}\n",
    "$\n",
    "\n",
    "- take the derivative\n",
    "\n",
    "$\n",
    "\\displaystyle l(\\theta) = \\sum_{i=1}^N \\{x_i\\log\\theta + (1-x_i)\\log(1-\\theta)\\} \\\\\n",
    "\\displaystyle \\frac{dl}{d\\theta} = \\frac{1}{\\theta}\\sum_{i=1}^Nx_i - \\frac{1}{1-\\theta}\\sum_{i=1}^N(1-x_i)\n",
    "$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
